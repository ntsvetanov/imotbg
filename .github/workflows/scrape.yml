name: Property Listings Scraping

on:
  schedule:
    - cron: "0 6,18 * * *"
  workflow_dispatch:

concurrency:
  group: property-scraping
  cancel-in-progress: false

jobs:
  scrape-sites:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    environment:
      name: prod
      url: https://github.com
    strategy:
      fail-fast: false
      matrix:
        scraper:
          - name: ImotBg
            folder: imotbg
          - name: ImotiNet
            folder: imotinet
          - name: HomesBg
            folder: homesbg
    steps:
      - name: Check out this repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          sparse-checkout: |
            src
            requirements.txt
            main.py
            config.py
            url_configs.json

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Create data directory
        run: mkdir -p results

      - name: Install requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        env:
          MAILTRAP_HOST: ${{ vars.MAILTRAP_HOST }}
          MAILTRAP_SENDER_EMAIL: ${{ vars.MAILTRAP_SENDER_EMAIL }}
          MAILTRAP_SEND_TO_EMAIL: ${{ secrets.MAILTRAP_SEND_TO_EMAIL }}
          MAILTRAP_TOKEN: ${{ secrets.MAILTRAP_TOKEN }}
        run: python main.py --scraper_name "${{ matrix.scraper.name }}" --result_folder results

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.scraper.name }}
          # Upload only the subtrees we care about, BUT without duplicating "results/" later
          # This keeps the artifact root as "raw/..." and "processed/..."
          path: |
            results/raw/${{ matrix.scraper.folder }}/
            results/processed/${{ matrix.scraper.folder }}/
          retention-days: 1
          if-no-files-found: ignore

  commit-results:
    needs: scrape-sites
    if: always()
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Check out this repo (full history, sparse - only results folder)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          sparse-checkout: |
            results

      - name: Download all artifacts into results/
        uses: actions/download-artifact@v4
        with:
          path: results/
          pattern: results-*
          merge-multiple: true

      - name: Commit and push only changed results
        run: |
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"

          git pull --rebase

          git add results/
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          git commit -m "Scraper results: ${timestamp}"
          git push
