name: Property Listings Scraping

on:
  schedule:
    - cron: "0 6,18 * * *"
  workflow_dispatch:

concurrency:
  group: property-scraping
  cancel-in-progress: false

jobs:
  scrape-sites:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    environment:
      name: prod
      url: https://github.com
    strategy:
      fail-fast: false
      matrix:
        scraper:
          - name: AloBg
            folder: alobg
          - name: BazarBg
            folder: bazarbg
          - name: BulgarianProperties
            folder: bulgarianproperties
          - name: HomesBg
            folder: homesbg
          - name: ImotBg
            folder: imotbg
          - name: ImotiCom
            folder: imoticom
          - name: ImotiNet
            folder: imotinet
          - name: ImotiNetPlovdiv
            folder: imotinet
          - name: Luximmo
            folder: luximmo
          - name: Suprimmo
            folder: suprimmo
    steps:
      - name: Check out this repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          sparse-checkout: |
            src
            requirements.txt
            main.py
            config.py
            url_configs.json

      - name: Set date variables
        id: date
        run: |
          echo "year=$(date -u +%Y)" >> $GITHUB_OUTPUT
          echo "month=$(date -u +%m)" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Create data directory
        run: mkdir -p results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}

      - name: Install requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download raw data
        run: python main.py download --site "${{ matrix.scraper.name }}" --result_folder results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}

      - name: Process downloaded data
        run: python main.py process --site "${{ matrix.scraper.name }}" --result_folder results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.scraper.name }}
          path: |
            results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}/raw/${{ matrix.scraper.folder }}/
            results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}/raw_html/${{ matrix.scraper.folder }}/
            results/${{ steps.date.outputs.year }}/${{ steps.date.outputs.month }}/processed/${{ matrix.scraper.folder }}/
          retention-days: 1
          if-no-files-found: ignore

  commit-results:
    needs: scrape-sites
    if: always()
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Check out this repo (full history, sparse - only results folder)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          sparse-checkout: |
            results

      - name: Download all artifacts into results/
        uses: actions/download-artifact@v4
        with:
          path: results/
          pattern: results-*
          merge-multiple: true

      - name: Commit and push only changed results
        run: |
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"

          git pull --rebase

          git add results/
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          git commit -m "Scraper results: ${timestamp}"
          git push
